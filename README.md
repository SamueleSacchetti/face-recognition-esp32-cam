# Face Recognition using ESP32-Cam

ESP32-Cam is an edge device with low memory capacity and low computational performance. It can be used for many tasks such as automation, security and surveillance, and so on. In this project, we used the ESP32-Cam to perform real-time inference using a pre-trained face recognition model, handling the output with LEDs based on the result obtained.

## Failed Steps Before the Success

### First Approach
We started with a "manual approach", performing a fine-tuning process on the MobileNetV2 model. The dataset used was composed of our faces and unknown peopleâ€™s faces pictures. During the implementation (using Micropython as the programming language and Thonny as the IDE), we faced two main problems:
1. The model we obtained was too big to save and run on the device.
2. There is no TensorFlow Lite library for Micropython.

This solution can be found [here](./python_customModel).

In [this directory](./python_customModel/converted_models) there are two different .tflite files: [model.tflite](./python_customModel/converted_models/model.tflite) contains the model converted in TensorFlow Lite (tflite) and [quantized_model.tflite](./python_customModel/converted_models/quantized_model.tflite) contains the quantized model always as a tflite model.
In [this directory](./python_customModel/fine_tuning) there is the script python that we used to fine tune the MobileNetV2 model.
In [this directory](./python_customModel/saved_model) there is the saved model after the fine tuning process.
[This file](./python_customModel/modelConverter.py) contains the script that we used to convert a saved_model into a quantized_model in .tflite. This conversion has been possible thanks to the TFLiteConverter class.

### Second Approach
After the experience with Micropython and TensorFlow Lite, we converted the saved_model.pb file into a C byte array so we could upload the model on the device in an easier way. This solved the second problem from the first approach. However, the first problem remained due to the model's size. Then we found a "person detection" model provided by TensorFlow Lite. The model's output is a confidence score indicating if a person is in front of the camera or not. In this second approach, we used Arduino as the IDE, but the results were not satisfying due to the low accuracy.

This second solution can be found [here](./tflite_arduino_person_detection).
In [this directory](face-recognition-esp32-cam/tflite_arduino_person_detection/person_detection) there are all the libraries and files included in the main script such as:
  1. The library for the camera configuration
  2. The [model](face-recognition-esp32-cam/tflite_arduino_person_detection/person_detection/person_detect_model_data.cpp) used for the inference.

### Final Approach: The Success!
Despite these failures, we learned a lot from the experiences. We decided to try another approach using the tool Edge Impulse. We rebuilt the dataset from scratch using Roboflow's tools. To avoid overfitting, we added more pictures and performed data augmentation. Then, we fine-tuned MobileNetv2 on Edge Impulse, which produced the model and automatically converted it to TensorFlow Lite and then in C byte array. Edge Impulse also generated the basic C code needed to run the model inference on the ESP32.

This final solution can be found [here](./arduino_edge_impulse).

## Inference of the Face Recognition Model
We modified the code generated by Edge Impulse to suit our needs. Specifically, we configured the pins to connect the SD card and the camera. To manage the output and activate the appropriate LED, we added a confidence threshold. When the threshold is exceeded, the LED corresponding to the classification result is turned on; otherwise, it is turned off.

## How to Replicate the Obtained Results
Here are the steps to replicate the final result:

1. Install and run Arduino IDE.
2. Install the "TensorFlowLite_ESP32" library on Arduino IDE using Library Manager.
3. Configure the camera, depending on the device used. We used the "CAMERA_MODEL_WROVER_KIT" module. The provided camera configuration for ESP32 can be found [here](https://github.com/espressif/esp32-camera).
4. Connect the device to the computer via USB.
5. Check the correct port and device in Arduino IDE settings.
6. Compile and run the [source code](./arduino_edge_impulse/examples/esp32/esp32_camera_2024/esp32_camera_2024.ino).

As you can see in the source code, you can define the pins where you can connect the different LEDs. Each LED is "connected" to a specific class predicted by the model.

```c
#define PIN_BLUE 2
#define PIN_GREEN 15
#define PIN_RED 32
```

The model used for the inference can be found [here](arduino_edge_impulse/src/tflite-model).
